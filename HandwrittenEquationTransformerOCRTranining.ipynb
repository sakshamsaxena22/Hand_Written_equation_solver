{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakshamsaxena22/Hand_Written_equation_solver/blob/main/HandwrittenEquationTransformerOCRTranining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmNjSwgwkDXT",
        "outputId": "a3265b92-0d99-41e9-9160-695e44bf513a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (2.0.8)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations) (6.0.3)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.12.3)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations) (4.12.0.88)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (4.5.1)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (6.5.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# Transformer OCR with Pretrained CNN Encoder (Equation Recognition)\n",
        "# Uses pretrained_cnn_encoder_32.pth from symbol pretraining\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 1. SETUP\n",
        "# =========================\n",
        "!pip install torch torchvision albumentations opencv-python tqdm\n",
        "\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG1r1ao11DxV",
        "outputId": "76c04d55-f4a7-42a1-cfb0-b01dc89216d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'hme100k' dataset.\n",
            "HME100K root: /kaggle/input/hme100k\n",
            "Images directory: /kaggle/input/hme100k\n",
            "Label file: /kaggle/input/hme100k/train.txt\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "DATASET_ROOT = kagglehub.dataset_download(\"cutedeadu/hme100k\")\n",
        "print(\"HME100K root:\", DATASET_ROOT)\n",
        "\n",
        "# IMPORTANT:\n",
        "# In this dataset, DATASET_ROOT already points to the folder containing images\n",
        "# There is NO nested images/ directory beyond this level\n",
        "\n",
        "# Detect label file dynamically\n",
        "LABEL_FILE = None\n",
        "for fname in os.listdir(DATASET_ROOT):\n",
        "    if fname.lower() in {\"labels.txt\", \"annotations.txt\", \"train.txt\"}:\n",
        "        LABEL_FILE = os.path.join(DATASET_ROOT, fname)\n",
        "        break\n",
        "\n",
        "assert LABEL_FILE is not None, \"No label file found (labels.txt / annotations.txt)\"\n",
        "\n",
        "IMG_DIR = DATASET_ROOT\n",
        "\n",
        "print(\"Images directory:\", IMG_DIR)\n",
        "print(\"Label file:\", LABEL_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "URmtjX8ZIsjW"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 3. CHARSET\n",
        "# =========================\n",
        "CHARSET = \"0123456789+-=*/()xyz\"\n",
        "char2idx = {c: i + 1 for i, c in enumerate(CHARSET)}  # 0 = CTC blank\n",
        "idx2char = {i: c for c, i in char2idx.items()}\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 4. DATASET CLASS\n",
        "# =========================\n",
        "# class EquationDataset(Dataset):\n",
        "#     def __init__(self, img_dir, label_file):\n",
        "#         self.img_dir = img_dir\n",
        "#         self.samples = []\n",
        "\n",
        "#         with open(label_file, 'r', encoding='utf-8') as f:\n",
        "#             for line in f:\n",
        "#                 parts = line.strip().split('\\t')\n",
        "#                 if len(parts) != 2:\n",
        "#                     continue\n",
        "#                 fname, label = parts\n",
        "#                 img_path = os.path.join(self.img_dir, fname)\n",
        "#                 if os.path.isfile(img_path):\n",
        "#                     self.samples.append((fname, label))\n",
        "\n",
        "#         assert len(self.samples) > 0, \"No valid image-label pairs found\"\n",
        "\n",
        "#         self.transform = transforms.Compose([\n",
        "#             transforms.ToTensor(),\n",
        "#             transforms.Normalize((0.5,), (0.5,))\n",
        "#         ])\n",
        "\n",
        "#         print(f\"Loaded {len(self.samples)} samples\")\n",
        "\n",
        "#     def encode(self, text):\n",
        "#         return torch.tensor([char2idx[c] for c in text if c in char2idx], dtype=torch.long)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.samples)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         fname, label = self.samples[idx]\n",
        "#         img = cv2.imread(os.path.join(self.img_dir, fname), cv2.IMREAD_GRAYSCALE)\n",
        "#         img = cv2.resize(img, (256, 64))\n",
        "#         img = self.transform(img)\n",
        "#         return img, self.encode(label), len(label)\n",
        "\n",
        "\n",
        "class EquationDataset(Dataset):\n",
        "    def __init__(self, img_dir, label_file):\n",
        "        self.img_dir = img_dir\n",
        "        self.samples = []\n",
        "\n",
        "        with open(label_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) != 2:\n",
        "                    continue\n",
        "\n",
        "                fname, label = parts\n",
        "                img_path = os.path.join(self.img_dir, fname)\n",
        "                if not os.path.isfile(img_path):\n",
        "                    continue\n",
        "\n",
        "                # Encode ONCE, here\n",
        "                encoded = [char2idx[c] for c in label if c in char2idx]\n",
        "\n",
        "                # Drop samples that collapse after filtering\n",
        "                if len(encoded) == 0:\n",
        "                    continue\n",
        "\n",
        "                self.samples.append((fname, torch.tensor(encoded, dtype=torch.long)))\n",
        "\n",
        "        assert len(self.samples) > 0, \"No valid samples after filtering\"\n",
        "        print(f\"Loaded {len(self.samples)} samples\")\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname, encoded = self.samples[idx]\n",
        "\n",
        "        img = cv2.imread(os.path.join(self.img_dir, fname), cv2.IMREAD_GRAYSCALE)\n",
        "        img = cv2.resize(img, (192, 64))\n",
        "        img = self.transform(img)\n",
        "\n",
        "        return img, encoded, encoded.numel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nbgKKOi50LBy"
      },
      "outputs": [],
      "source": [
        " #=========================\n",
        "# 5. PRETRAINED CNN ENCODER\n",
        "# =========================\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "i6cBsWpX0ONV"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 6. TRANSFORMER OCR MODEL\n",
        "# =========================\n",
        "class TransformerOCR(nn.Module):\n",
        "    def __init__(self, num_classes, d_model=256, nhead=8, num_layers=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn = CNNEncoder()\n",
        "        self.cnn.features.load_state_dict(\n",
        "            torch.load(\"pretrained_cnn_encoder_32.pth\", map_location=\"cpu\")\n",
        "        )\n",
        "\n",
        "        # Freeze CNN initially\n",
        "        for p in self.cnn.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.proj = nn.Linear(128 * 8, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)               # [B, 128, 8, W]\n",
        "        b, c, h, w = x.size()\n",
        "        x = x.permute(0, 3, 1, 2).contiguous().view(b, w, c * h)\n",
        "        x = self.proj(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.transformer(x)\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YyUYnbm00Sa8"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 7. POSITIONAL ENCODING\n",
        "# =========================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=1000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EN9aTX-U0WSJ"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 8. COLLATE FUNCTION\n",
        "# =========================\n",
        "def collate(batch):\n",
        "    imgs, labels, lens = zip(*batch)\n",
        "    imgs = torch.stack(imgs)\n",
        "    labels = torch.cat(labels)\n",
        "    target_lengths = torch.tensor(lens)\n",
        "    return imgs, labels, target_lengths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NaYnWJc0Z8o",
        "outputId": "c5393dfe-cc57-4792-c656-ab1109866f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 98086 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12261/12261 [09:22<00:00, 21.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 2.5354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12261/12261 [05:52<00:00, 34.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Loss: 1.9229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12261/12261 [05:53<00:00, 34.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Loss: 1.6821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12261/12261 [05:51<00:00, 34.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Loss: 1.5362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12261/12261 [05:55<00:00, 34.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Loss: 1.4394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12261/12261 [05:51<00:00, 34.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Loss: 1.3629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12261/12261 [05:49<00:00, 35.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Loss: 1.3055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12261/12261 [05:49<00:00, 35.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Loss: 1.2549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12261/12261 [05:49<00:00, 35.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 | Loss: 1.2182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12261/12261 [05:49<00:00, 35.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Loss: 1.1861\n",
            "Model saved\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# 9. TRAINING LOOP (FIXED)\n",
        "# =========================\n",
        "def train():\n",
        "    dataset = EquationDataset(IMG_DIR, LABEL_FILE)\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate\n",
        "    )\n",
        "\n",
        "    model = TransformerOCR(len(CHARSET) + 1).to(DEVICE)\n",
        "\n",
        "    # ‚úÖ CORRECT: zero_infinity belongs HERE\n",
        "    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=3e-4\n",
        "    )\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for imgs, targets, target_lengths in tqdm(loader):\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "            target_lengths = target_lengths.to(DEVICE)\n",
        "\n",
        "            # Forward\n",
        "            logits = model(imgs)\n",
        "            log_probs = logits.log_softmax(2)\n",
        "\n",
        "            # CTC input lengths (time dimension)\n",
        "            input_lengths = torch.full(\n",
        "                size=(log_probs.size(0),),\n",
        "                fill_value=log_probs.size(1),\n",
        "                dtype=torch.long,\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "            # ‚úÖ CORRECT: forward call has ONLY 4 arguments\n",
        "            loss = criterion(\n",
        "                log_probs.permute(1, 0, 2),  # (T, B, C)\n",
        "                targets,                     # (sum(target_lengths))\n",
        "                input_lengths,               # (B,)\n",
        "                target_lengths               # (B,)\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1} | Loss: {total_loss / len(loader):.4f}\")\n",
        "\n",
        "    torch.save(\n",
        "        model.state_dict(),\n",
        "        \"transformer_ocr_with_pretrained_cnn.pth\"\n",
        "    )\n",
        "    print(\"Model saved\")\n",
        "\n",
        "\n",
        "train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ctc_greedy_decode(log_probs, idx2char):\n",
        "    # log_probs: (T, C)\n",
        "    pred = log_probs.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "    decoded = []\n",
        "    prev = 0  # CTC blank = 0\n",
        "    for p in pred:\n",
        "        if p != prev and p != 0:\n",
        "            decoded.append(idx2char[p])\n",
        "        prev = p\n",
        "\n",
        "    return \"\".join(decoded)\n"
      ],
      "metadata": {
        "id": "9Xjr-Rq3E5CR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xZ-J3GAsFOtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model= TransformerOCR(len(CHARSET) + 1).to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"transformer_ocr_with_pretrained_cnn.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuKWAzFnFPOB",
        "outputId": "540fb20f-ed40-4790-f3c0-b562121e239a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recreate dataset for inference\n",
        "dataset = EquationDataset(IMG_DIR, LABEL_FILE)\n",
        "\n",
        "print(\"Dataset size:\", len(dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7ezyB7cF9LN",
        "outputId": "b1ecef81-ed9c-47c1-b0dc-78a1a8b7c782"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 98086 samples\n",
            "Dataset size: 98086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n4m3AnrrE_hT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# Take one sample\n",
        "img, label_encoded, _ = dataset[0]\n",
        "img = img.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(img)\n",
        "    log_probs = logits.log_softmax(2)[0]   # (T, C)\n",
        "\n",
        "prediction = ctc_greedy_decode(log_probs, idx2char)\n",
        "\n",
        "# Convert true label\n",
        "true_label = \"\".join([idx2char[i.item()] for i in label_encoded])\n",
        "\n",
        "print(\"Prediction :\", prediction)\n",
        "print(\"Ground Truth:\", true_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxzTQdREFANv",
        "outputId": "8c767468-b91e-456f-c912-6513451a1b67"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction : 22+=2+()2\n",
            "Ground Truth: (2)2+4=24+()2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "shl8Zq1eLEnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfreezing last layer of transformer+cnn ocr\n"
      ],
      "metadata": {
        "id": "PrEf5J2jGhvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 9. TRAINING LOOP (FINE-TUNING)\n",
        "# =========================\n",
        "def train_finetune():\n",
        "\n",
        "    # Dataset + Loader\n",
        "    dataset = EquationDataset(IMG_DIR, LABEL_FILE)\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate\n",
        "    )\n",
        "\n",
        "    # 1Ô∏è‚É£ Recreate model architecture\n",
        "    model = TransformerOCR(len(CHARSET) + 1).to(DEVICE)\n",
        "\n",
        "    # 2Ô∏è‚É£ Load trained weights\n",
        "    model.load_state_dict(\n",
        "        torch.load(\n",
        "            \"/content/transformer_ocr_with_pretrained_cnn.pth\",\n",
        "            map_location=DEVICE\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # 3Ô∏è‚É£ Freeze EVERYTHING first\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # 4Ô∏è‚É£ Unfreeze ONLY last Transformer block\n",
        "    for name, p in model.named_parameters():\n",
        "        if \"transformer.layers.3\" in name:   # last encoder layer\n",
        "            p.requires_grad = True\n",
        "\n",
        "    # Always keep classifier trainable\n",
        "    for p in model.classifier.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    # 5Ô∏è‚É£ Loss (correct usage)\n",
        "    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "\n",
        "    # 6Ô∏è‚É£ Optimizer with LOWER LR\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=1e-4\n",
        "    )\n",
        "\n",
        "    # üî• Fine-tune ONLY 1‚Äì2 epochs\n",
        "    EPOCHS = 2\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for imgs, targets, target_lengths in tqdm(loader):\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "            target_lengths = target_lengths.to(DEVICE)\n",
        "\n",
        "            logits = model(imgs)\n",
        "            log_probs = logits.log_softmax(2)\n",
        "\n",
        "            input_lengths = torch.full(\n",
        "                (log_probs.size(0),),\n",
        "                log_probs.size(1),\n",
        "                dtype=torch.long,\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "            loss = criterion(\n",
        "                log_probs.permute(1, 0, 2),\n",
        "                targets,\n",
        "                input_lengths,\n",
        "                target_lengths\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Fine-tune Epoch {epoch + 1} | Loss: {total_loss / len(loader):.4f}\")\n",
        "\n",
        "    # 7Ô∏è‚É£ Save updated model\n",
        "    torch.save(\n",
        "        model.state_dict(),\n",
        "        \"/content/transformer_ocr_with_pretrained_cnn_finetuned.pth\"\n",
        "    )\n",
        "\n",
        "    print(\"Fine-tuned model saved\")\n",
        "\n",
        "\n",
        "train_finetune()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlC5rejIGzrI",
        "outputId": "3da6e4b4-939f-4931-f393-b90ee762c546"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 98086 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12261/12261 [04:41<00:00, 43.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tune Epoch 1 | Loss: 1.0805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12261/12261 [04:39<00:00, 43.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tune Epoch 2 | Loss: 1.0575\n",
            "Fine-tuned model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= TransformerOCR(len(CHARSET) + 1).to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"transformer_ocr_with_pretrained_cnn_finetuned.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PalBgPxqLF7p",
        "outputId": "9febd1ae-256b-4e98-ec45-32212ee92194"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# Take one sample\n",
        "img, label_encoded, _ = dataset[0]\n",
        "img = img.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(img)\n",
        "    log_probs = logits.log_softmax(2)[0]   # (T, C)\n",
        "\n",
        "prediction = ctc_greedy_decode(log_probs, idx2char)\n",
        "\n",
        "# Convert true label\n",
        "true_label = \"\".join([idx2char[i.item()] for i in label_encoded])\n",
        "\n",
        "print(\"Prediction :\", prediction)\n",
        "print(\"Ground Truth:\", true_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGyrbtt0LGhi",
        "outputId": "18d9a939-3169-4ab3-8131-f0478710f192"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction : 22+=2+()2\n",
            "Ground Truth: (2)2+4=24+()2\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMLD9jJB8VhWifunr7R+teB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}